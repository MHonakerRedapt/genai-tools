{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad68c8a-0dec-4a65-9de1-975aef7b9aa8",
   "metadata": {},
   "source": [
    "# General Purpose Document Parsing and Index Creation Notebook for Generative AI PoC Projects\n",
    "Notebook and associated modules for parsing source documents and index creation Generative AI projects. It is recommended to use this notebook to evaluate the parsing output on a set of sample documents prior to attemping to parse the full document library. Often the parsing functions will need to modified directly to ensure the best possible parsing results.\n",
    "\n",
    "### ToDo\n",
    "* Add functionality for other cloud providers\n",
    "* Add additional index types\n",
    "* Add csv/excel parsing\n",
    "* update to allow AML pipelines for document processing (for large numbers of files)\n",
    "* create Bicep or TF files for resource deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9e8ce-ecae-473b-99f4-c5c57aef3011",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "It is recommended to set up a python virtual environment in which to install the needed packages, and work from there. Most of this notebook assumes a Linux OS, but most items will work from Windows OS also. The exceptions are converting Word documents into PDFs, which must be done in Windows (see example Powershell script), or if extracting text from Word documents directly, the utilities assume Bash is the standard OS commandline interface.\n",
    "\n",
    "To create a new virtual environment, run one of the following commands from the command line (bash or cmd)  \n",
    "Create a virtual environment - Linux:  \n",
    "`$ python -m venv /path/to/new/virtual/environment`  \n",
    "\n",
    "Create a virtual environment - Windows:  \n",
    "if PATH and PATHEXT are configured:  \n",
    "`c:\\>python -m venv c:\\path\\to\\new\\virtual\\environment`  \n",
    "or if not:  \n",
    "`c:\\>replace with python path\\python -m venv c:\\path\\to\\new\\virtual\\environment`\n",
    "\n",
    "A new directory will be created, if it does not exist.\n",
    "\n",
    "Activate the virtual environment (where env is repaced with the path to your virtual environment) - Linux:  \n",
    "`$ source env/bin/activate`  \n",
    "Activate the virtual environment (where env is repaced with the path to your virtual environment) - Windows:  \n",
    "`c:\\> .\\env\\Scripts\\activate`\n",
    "\n",
    "Once you are in your virtual environment, install the necessary packages, shown in Pipfile for example:  \n",
    "`python -m pipenv install`  \n",
    "the exact command depends on your system and preferred python package manager\n",
    "\n",
    "To leave the virtual environment, just run the command `deactivate`\n",
    "\n",
    "\n",
    "Additional references:  \n",
    "https://docs.python.org/3/library/venv.html#venv-def  \n",
    "https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/#creating-a-virtual-environmentv  \n",
    "https://detox.sourceforge.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63368a3e-4de8-4c34-834a-123ecb6f4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from data_utils import chunk_directory, FILE_FORMAT_DICT\n",
    "\n",
    "# optional imports depending on index method\n",
    "import weaviate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380f2d1-15fd-4e96-8cd4-d543316f9f9d",
   "metadata": {},
   "source": [
    "## Dowload files from an Azure Blobstore\n",
    "* To explicitly download the docuuments from Azure blob store, you can use the cell below by:\n",
    "* Add the key and container names - this will download all blobs from the containers.\n",
    "* If you want to limit the files to specific extensions, update the logic in the function below or run in a loop for each file extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bb913-2704-46d8-9e90-ce799ecc71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "dowload_blobs = False\n",
    "blob_connection_string = '<replace with blob connection string>'\n",
    "blob_containers = ['<replace with blob container names>']\n",
    "\n",
    "def get_blobs(blob_service_client = None, blob_containers = None, file_extensions = None):\n",
    "    for container in blob_containers:\n",
    "        container_client = blob_service_client.get_container_client(container)\n",
    "        blob_list = container_client.list_blobs()\n",
    "\n",
    "        if not os.path.exists('data/'):\n",
    "            os.makedirs('data/')\n",
    "        for blob in blob_list:\n",
    "            with open('data/' + re.split('/', blob.name[-1]) as f:\n",
    "                f.write(container_client.download_blob(blob.name).readall())\n",
    "\n",
    "if download_blobs:\n",
    "    # setup blob connection\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "    get_blobs(blob_service_client=blob_service_client, blob_containers=blob_containers, file_extensions=file_extensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7391b74-9d32-40f9-9255-5c3ca07765c8",
   "metadata": {},
   "source": [
    "## Parse and Examine Source Documents (locally)\n",
    "Parse a set of files locally. This will not create a vector index or do any embedding. It is recommended to use this method on at least a few test files of each type that you want to parse, and examine the output. In many, if not most, cases the parsing will need to have some tuning to parse well and cleanly.\n",
    "* Set and/or uncomment the appropiate variables below\n",
    "* If you want to use Azure Form Recognizer to crack the PDFs (if any) then supply the appropriate credentials and information.\n",
    "* * Export the form recognizer endpoint and key to your environment\n",
    "* * If the source documents (pdf) contain a lot of formatted data, e.g., lots of tables, then it is recommended to create a specific layout in the form recognizer to use\n",
    " \n",
    "### Notes:\n",
    "* Start with approximately a 10% token overlap (e.g., if the num_tokens = 1024, the token_overlap should be set to about 128 (as token counts are normally in multiples of 16)\n",
    "* increasing njobs may help speed things up if there are many files to process. Max is 32\n",
    "* Currently, adding vectors to Azure Cognitive Seach index during parsing does NOT work well on the Azure end. It is NOT recommended attempt to embed vectors if using this service. Investigations are underway to support this is a more reliable fashion\n",
    "* If Azure Document Intelligence is to be used for parsing, it does NOT support Word (.doc, .docx) documents. They must be converted to PDF. See the Powershell script below to do this\n",
    "* If using the shell script to parse Word documents in a Linux OS, the the file paths needd to be cleaned up (e.g., no spaces or diacritical marks) the detox utility is recommended. See references above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b85edc25-a46f-4c85-8116-a52d30ba215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and uncomment these variables if you want to use Azure Form Recognizer to crack pdfs, \n",
    "# otherwise the script will use pypdf to crack pdf files\n",
    "# os.environ[\"FORM_RECOGNIZER_ENDPOINT\"] = None\n",
    "# os.environ[\"FORM_RECOGNIZER_KEY\"] = None\n",
    "\n",
    "# you must set the directory path. The other variables are optional,\n",
    "# and are listed with their default values, the extensions list includes txt, html, pdf, py, md files\n",
    "# Word files are supported, but some prior preparation is needed\n",
    "directory_path = '<top_level_directory_containing_documents>'\n",
    "ignore_errors = True\n",
    "num_tokens = 1024\n",
    "min_chunk_size = 10\n",
    "url_prefix = None\n",
    "token_overlap = 128\n",
    "extensions_to_process = list(FILE_FORMAT_DICT.keys())\n",
    "form_recognizer_client = None\n",
    "use_layout = False\n",
    "njobs=4\n",
    "add_embeddings = False\n",
    "azure_credential = None\n",
    "embedding_endpoint = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32099d7-4f7a-448f-855c-0401795017b3",
   "metadata": {},
   "source": [
    "If you want to used Azure Document Intelligence to process Word files (.doc, .docx) you will need to convert them to pdf files first. An example powershell script is shown below.\n",
    "```\n",
    "Folders = Get-ChildItem <directory-name-here> -Directory -Recurse\n",
    "\n",
    " ForEach ($Folder in $Folders)\n",
    " {\n",
    "     $wdFormatPDF = 17\n",
    "     $word = New-Object -ComObject word.application\n",
    "     $word.visible = $false\n",
    "     $folderpath = \"$($Folder.FullName)\\*\"\n",
    "    $fileTypes = \"*.docx\",\"*doc\"\n",
    "    Get-ChildItem -path $folderpath -include $fileTypes |\n",
    "    foreach-object `\n",
    "     {\n",
    "    $path =  ($_.fullname).substring(0,($_.FullName).lastindexOf(\".\"))\n",
    "    \"Converting $path to pdf ...\"\n",
    "     $doc = $word.documents.open($_.fullname)\n",
    "     $doc.saveas([ref] $path, [ref]$wdFormatPDF)\n",
    "     $doc.close()\n",
    "    }\n",
    "    $word.Quit()\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8e2f7-3a61-4b54-8468-b9d339962e5e",
   "metadata": {},
   "source": [
    "### Parse and Chunk the Documents\n",
    "* Running this function will recursively parse and chunk all the files in the supplied directory_path and it's subdirectories\n",
    "* A chunking result will be returned - which consists of an object with the folling attributes:\n",
    "* * chunks (List[Document]): List of chunks\n",
    "* * total_files (int): Total number of files.\n",
    "* * num_unsupported_format_files (int): Number of files with unsupported format.\n",
    "* * num_files_with_errors (int): Number of files with errors.\n",
    "* * skipped_chunks (int): Number of chunks skipped.\n",
    "\n",
    "\n",
    "* The chunks will be a list of objects with type Document, with the following attributes:\n",
    "* * content (str): The content of the document.\n",
    "* * id (Optional[str]): The id of the document.\n",
    "* * title (Optional[str]): The title of the document.\n",
    "* * filepath (Optional[str]): The filepath of the document.\n",
    "* * url (Optional[str]): The url of the document.\n",
    "* * metadata (Optional[Dict]): The metadata of the document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2edb8c81-7290-4f6f-b80f-ea0d07af18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_directory(\n",
    "    directory_path=directory_path,\n",
    "    ignore_errors=ignore_errors,\n",
    "    num_tokens=num_tokens,\n",
    "    min_chunk_size=min_chunk_size,\n",
    "    url_prefix=url_prefix,\n",
    "    token_overlap=token_overlap,\n",
    "    extensions_to_process=extensions_to_process,\n",
    "    form_recognizer_client=form_recognizer_client,\n",
    "    use_layout=use_layout,\n",
    "    njobs=njobs,\n",
    "    add_embeddings=add_embeddings,\n",
    "    azure_credential=azure_credential,\n",
    "    embedding_endpoint=embedding_endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fb951-094c-472f-a4dc-e2ae4d893247",
   "metadata": {},
   "source": [
    "## Create Index and/or Vector Store\n",
    "In order for the information contained in the source documents to be accessible and relevant to Generative AI, then these parsed out chunks of text must be used to create an index of some sort. Examples below illustrate using Azure Cognitive Search and Weaviate as the searchable index.\n",
    "\n",
    "If possible, a separate index should be created for each language represented in the source documents.\n",
    "\n",
    "Important: ensure that there are no duplicate documents before index creation. This leads to poor search results.\n",
    "\n",
    "### Choosing between Azure Cognitive Search and Weaviate\n",
    "Azure Cognitive Search integrates better with other Azure services for some use cases. In particular, if certain fields need to be restricted based on Azure authentication, or if an ETL pipline feeding off an Azure storage method are anticipated, then this may be the better choice. It is also slower and more expensive, and if a very large number of documents are to be inserted into the index, then the pricing tier needs to be upgraded. Weaviate is open source and very fast, and allows easier customization on how the data stored and retrieved. It is also deployed on Kubernetes, making it cloud/on-prem agnostic. It is limited to BM25 keyword and vector searching, although it does allow filtering on metatdata and fields.\n",
    "\n",
    "Both allow extensive customization of the index fields, including searchability, formatting, types, restrictions, filtering, and data returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f1753-b074-45f6-b24c-4087a1b19c3e",
   "metadata": {},
   "source": [
    "### Azure Cognitive Search\n",
    "Azure Cognitive Search can be used as the backing index for Generative AI. Azure Cognitive Search supports serval types of searches, depending somewhat on how the index is built. It supports full text search - using Lucerne; vector search - if embedding vectors are created during index creation; hybrid search - a combination of full text and vector search; and other - not normally used directly by Generative AI applications, but useful for things like geospatial search, or complex fielded searching. See https://learn.microsoft.com/en-us/azure/search/ for additional information on how Azure Cognitive Search works and the various options allowed.\n",
    "\n",
    "Once the parsing and chunking operations are tuned, bulk document parsing and index creation can be completed:\n",
    "* Create a config file like `config.json`. You can create multiple indices for various configurations and data sets concurrently. The format of `config.json` should be an array of JSON objects, with each object specifying a configuration of local (or Azure blob store) data path and target search service and index. If the search service or index does not exist, it will be created. You can insert additional items into the index at any point after the initial creation. A sample `config.json` file is provided, with a single object for creation of one index and from one data path (can contain subdirectories). Your `config.json` file should look similiar to the example below:\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"data_path\": \"<local path or blob URL>\",\n",
    "        \"location\": \"<azure region, e.g. 'westus2'>\", \n",
    "        \"subscription_id\": \"<subscription id>\",\n",
    "        \"resource_group\": \"<resource group name>\",\n",
    "        \"search_service_name\": \"<search service name to use or create>\",\n",
    "        \"index_name\": \"<index name to use or create>\",\n",
    "        \"chunk_size\": 1024, // set to null to disable chunking before ingestion\n",
    "        \"token_overlap\": 128 // number of tokens to overlap between chunks\n",
    "        \"semantic_config_name\": \"default\",\n",
    "        \"language\": \"en\" // setting to set language of your documents. Change if your documents are not in English. Look in data_preparation_acs.py for SUPPORTED_LANGUAGE_CODES,\n",
    "        \"vector_config_name\": \"default\" // used if adding vectors to index - NOT recommended at this time.\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "The data_path can be a local directory, or an Azure Blob URL (like `https://<storage account name>.blob.core.windows.net/<container name>/<path>/`) If using a blob store, the files will be download to a temporary directory on your local machine.\n",
    "\n",
    "Run one of the following commands in your python virtual environment:  \n",
    "`python data_preparation_acs.py --config config.json --njobs=4` (or however many concurrent jobs you would like, max=32)  \n",
    "Or if using Azure Document Intelligence (Form Recognizer) for cracking PDF files:  \n",
    "`python data_preparation_acs.py --config.json --njobs=4 --form-rec-resource <form-rec-resource-name> --form-rec-key <form-rec-key>`  \n",
    "which will use the Form Recognizer Read model by default. If you have a layout, then you can pass in the argument `--form-rec-use-layout`\n",
    "\n",
    "Notes:\n",
    "* See the list in data_preparation_acs.py for supported language codes\n",
    "* It is NOT recommeded to use this process for adding vectors to an ACS index at this time. This service is in preview, and does not work well.\n",
    "* You may have to turn on sematic search (if using) through the Azure Portal. The REST interrface for ACS management does not always activate semantic search, however it is generally recommended to use semantic search for most GenAI applications for better search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01605ecf-19df-4e57-ac23-265c0c82b78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
